{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Chapter 1</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Positional and labeled indexing</h2>\n",
    "\n",
    "Given a pair of label-based indices, sometimes it's necessary to find the corresponding positions. In this exercise, you will use the Pennsylvania election results again. The DataFrame is provided for you as election.\n",
    "\n",
    "Find x and y such that election.iloc[x, y] == election.loc['Bedford', 'winner']. That is, what is the row position of 'Bedford', and the column position of 'winner'? Remember that the first position in Python is 0, not 1!\n",
    "\n",
    "To answer this question, first explore the DataFrame using election.head() in the IPython Shell and inspect it with your eyes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "election = pd.read_csv('data/pennsylvania2012_turnout.csv', index_col='county')\n",
    "\n",
    "# Assign the row position of election.loc['Bedford']: x\n",
    "x = 4\n",
    "\n",
    "# Assign the column position of election['winner']: y\n",
    "y = 4\n",
    "\n",
    "# Print the boolean equivalence\n",
    "print(election.iloc[x, y] == election.loc['Bedford', 'winner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! Depending on the situation, you may wish to use .iloc[] over .loc[], and vice versa. The important thing to realize is you can achieve the exact same results using either approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Indexing and column rearrangement</h2>\n",
    "\n",
    "There are circumstances in which it's useful to modify the order of your DataFrame columns. We do that now by extracting just two columns from the Pennsylvania election results DataFrame.\n",
    "\n",
    "Your job is to read the CSV file and set the index to 'county'. You'll then assign a new DataFrame by selecting the list of columns ['winner', 'total', 'voters']. The CSV file is provided to you in the variable filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           winner   total  voters\n",
      "county                           \n",
      "Adams      Romney   41973   61156\n",
      "Allegheny   Obama  614671  924351\n",
      "Armstrong  Romney   28322   42147\n",
      "Beaver     Romney   80015  115157\n",
      "Bedford    Romney   21444   32189\n"
     ]
    }
   ],
   "source": [
    "# Create a separate dataframe with the columns ['winner', 'total', 'voters']: results\n",
    "results = election[['winner', 'total', 'voters']]\n",
    "\n",
    "# Print the output of results.head()\n",
    "print(results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful work! The original election DataFrame had 6 columns, but as you can see, your results DataFrame now has just the 3 columns: 'winner', 'total', and 'voters'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Slicing rows</h2>\n",
    "The Pennsylvania US election results data set that you have been using so far is ordered by county name. This means that county names can be sliced alphabetically. In this exercise, you're going to perform slicing on the county names of the election DataFrame from the previous exercises, which has been pre-loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             state   total      Obama     Romney  winner   voters    turnout  \\\n",
      "county                                                                         \n",
      "Perry           PA   18240  29.769737  68.591009  Romney    27245  66.948064   \n",
      "Philadelphia    PA  653598  85.224251  14.051451   Obama  1099197  59.461407   \n",
      "Pike            PA   23164  43.904334  54.882576  Romney    41840  55.363289   \n",
      "Potter          PA    7205  26.259542  72.158223  Romney    10913  66.022175   \n",
      "\n",
      "                 margin  \n",
      "county                   \n",
      "Perry         38.821272  \n",
      "Philadelphia  71.172800  \n",
      "Pike          10.978242  \n",
      "Potter        45.898681  \n",
      "             state   total      Obama     Romney  winner   voters    turnout  \\\n",
      "county                                                                         \n",
      "Potter          PA    7205  26.259542  72.158223  Romney    10913  66.022175   \n",
      "Pike            PA   23164  43.904334  54.882576  Romney    41840  55.363289   \n",
      "Philadelphia    PA  653598  85.224251  14.051451   Obama  1099197  59.461407   \n",
      "Perry           PA   18240  29.769737  68.591009  Romney    27245  66.948064   \n",
      "\n",
      "                 margin  \n",
      "county                   \n",
      "Potter        45.898681  \n",
      "Pike          10.978242  \n",
      "Philadelphia  71.172800  \n",
      "Perry         38.821272  \n"
     ]
    }
   ],
   "source": [
    "# Slice the row labels 'Perry' to 'Potter': p_counties\n",
    "p_counties = election.loc['Perry':'Potter']\n",
    "\n",
    "# Print the p_counties DataFrame\n",
    "print(p_counties)\n",
    "\n",
    "# Slice the row labels 'Potter' to 'Perry' in reverse order: p_counties_rev\n",
    "p_counties_rev = election.loc['Potter':'Perry':-1]\n",
    "\n",
    "# Print the p_counties_rev DataFrame\n",
    "print(p_counties_rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Slicing columns</h2>\n",
    "\n",
    "Similar to row slicing, columns can be sliced by value. In this exercise, your job is to slice column names from the Pennsylvania election results DataFrame using .loc[].\n",
    "\n",
    "It has been pre-loaded for you as election, with the index set to 'county'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          state   total      Obama\n",
      "county                            \n",
      "Adams        PA   41973  35.482334\n",
      "Allegheny    PA  614671  56.640219\n",
      "Armstrong    PA   28322  30.696985\n",
      "Beaver       PA   80015  46.032619\n",
      "Bedford      PA   21444  22.057452\n",
      "               Obama     Romney  winner\n",
      "county                                 \n",
      "Adams      35.482334  63.112001  Romney\n",
      "Allegheny  56.640219  42.185820   Obama\n",
      "Armstrong  30.696985  67.901278  Romney\n",
      "Beaver     46.032619  52.637630  Romney\n",
      "Bedford    22.057452  76.986570  Romney\n",
      "              Romney  winner  voters    turnout     margin\n",
      "county                                                    \n",
      "Adams      63.112001  Romney   61156  68.632677  27.629667\n",
      "Allegheny  42.185820   Obama  924351  66.497575  14.454399\n",
      "Armstrong  67.901278  Romney   42147  67.198140  37.204293\n",
      "Beaver     52.637630  Romney  115157  69.483401   6.605012\n",
      "Bedford    76.986570  Romney   32189  66.619031  54.929118\n"
     ]
    }
   ],
   "source": [
    "# Slice the columns from the starting column to 'Obama': left_columns\n",
    "left_columns = election.loc[:, :'Obama']\n",
    "\n",
    "# Print the output of left_columns.head()\n",
    "print(left_columns.head())\n",
    "\n",
    "# Slice the columns from 'Obama' to 'winner': middle_columns\n",
    "middle_columns = election.loc[:, 'Obama':'winner']\n",
    "\n",
    "# Print the output of middle_columns.head()\n",
    "print(middle_columns.head())\n",
    "\n",
    "# Slice the columns from 'Romney' to the end: 'right_columns'\n",
    "right_columns = election.loc[:, 'Romney':]\n",
    "\n",
    "# Print the output of right_columns.head()\n",
    "print(right_columns.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Subselecting DataFrames with lists</h2>\n",
    "\n",
    "You can use lists to select specific row and column labels with the .loc[] accessor. In this exercise, your job is to select the counties ['Philadelphia', 'Centre', 'Fulton'] and the columns ['winner','Obama','Romney'] from the election DataFrame, which has been pre-loaded for you with the index set to 'county'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              winner      Obama     Romney\n",
      "county                                    \n",
      "Philadelphia   Obama  85.224251  14.051451\n",
      "Centre        Romney  48.948416  48.977486\n",
      "Fulton        Romney  21.096291  77.748861\n"
     ]
    }
   ],
   "source": [
    "# Create the list of row labels: rows\n",
    "rows = ['Philadelphia', 'Centre', 'Fulton']\n",
    "\n",
    "# Create the list of column labels: cols\n",
    "cols = ['winner', 'Obama', 'Romney']\n",
    "\n",
    "# Create the new DataFrame: three_counties\n",
    "three_counties = election.loc[rows,cols]\n",
    "\n",
    "# Print the three_counties DataFrame\n",
    "print(three_counties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent work! If you know exactly which rows and columns are of interest to you, this is a useful approach for subselecting DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Thresholding data</h2>\n",
    "\n",
    "In this exercise, we have provided the Pennsylvania election results and included a column called 'turnout' that contains the percentage of voter turnout per county. Your job is to prepare a boolean array to select all of the rows and columns where voter turnout exceeded 70%.\n",
    "\n",
    "As before, the DataFrame is available to you as election with the index set to 'county'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             state   total      Obama     Romney  winner  voters    turnout  \\\n",
      "county                                                                        \n",
      "Bucks           PA  319407  49.966970  48.801686   Obama  435606  73.324748   \n",
      "Butler          PA   88924  31.920516  66.816607  Romney  122762  72.436096   \n",
      "Chester         PA  248295  49.228539  49.650617  Romney  337822  73.498766   \n",
      "Forest          PA    2308  38.734835  59.835355  Romney    3232  71.410891   \n",
      "Franklin        PA   62802  30.110506  68.583803  Romney   87406  71.850903   \n",
      "Montgomery      PA  401787  56.637223  42.286834   Obama  551105  72.905708   \n",
      "Westmoreland    PA  168709  37.567646  61.306154  Romney  238006  70.884347   \n",
      "\n",
      "                 margin  \n",
      "county                   \n",
      "Bucks          1.165284  \n",
      "Butler        34.896091  \n",
      "Chester        0.422079  \n",
      "Forest        21.100520  \n",
      "Franklin      38.473297  \n",
      "Montgomery    14.350390  \n",
      "Westmoreland  23.738508  \n"
     ]
    }
   ],
   "source": [
    "# Create the boolean array: high_turnout\n",
    "high_turnout = election.turnout > 70\n",
    "\n",
    "# Filter the election DataFrame with the high_turnout array: high_turnout_df\n",
    "high_turnout_df = election[high_turnout]\n",
    "\n",
    "# Print the high_turnout_results DataFrame\n",
    "print(high_turnout_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Filtering columns using other columns</h2>\n",
    "\n",
    "The election results DataFrame has a column labeled 'margin' which expresses the number of extra votes the winner received over the losing candidate. This number is given as a percentage of the total votes cast. It is reasonable to assume that in counties where this margin was less than 1%, the results would be too-close-to-call.\n",
    "\n",
    "Your job is to use boolean selection to filter the rows where the margin was less than 1. You'll then convert these rows of the 'winner' column to np.nan to indicate that these results are too close to declare a winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 67 entries, Adams to York\n",
      "Data columns (total 8 columns):\n",
      "state      67 non-null object\n",
      "total      67 non-null int64\n",
      "Obama      67 non-null float64\n",
      "Romney     67 non-null float64\n",
      "winner     64 non-null object\n",
      "voters     67 non-null int64\n",
      "turnout    67 non-null float64\n",
      "margin     67 non-null float64\n",
      "dtypes: float64(4), int64(2), object(2)\n",
      "memory usage: 7.2+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vinicius\\AppData\\Local\\conda\\conda\\envs\\keras\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Create the boolean array: too_close\n",
    "too_close = election.margin < 1\n",
    "\n",
    "# Assign np.nan to the 'winner' column where the results were too close to call\n",
    "election['winner'][too_close] = np.nan\n",
    "\n",
    "# Print the output of election.info()\n",
    "print(election.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Filtering using NaNs</h2>\n",
    "In certain scenarios, it may be necessary to remove rows and columns with missing data from a DataFrame. The .dropna() method is used to perform this action. You'll now practice using this method on a dataset obtained from Vanderbilt University, which consists of data from passengers on the Titanic.\n",
    "\n",
    "The DataFrame has been pre-loaded for you as titanic. Explore it in the IPython Shell and you will note that there are many NaNs. You will focus specifically on the 'age' and 'cabin' columns in this exercise. Your job is to use .dropna() to remove rows where any of these two columns contains missing data and rows where all of these two columns contain missing data.\n",
    "\n",
    "You'll also use the .shape attribute, which returns the number of rows and columns in a tuple from a DataFrame, or the number of rows from a Series, to see the effect of dropping missing values from a DataFrame.\n",
    "\n",
    "Finally, you'll use the thresh= keyword argument to drop columns from the full dataset that have less than 1000 non-missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['pclass', 'survived', 'name', 'sex', 'age', 'sibsp', 'parch', 'ticket',\n",
      "       'fare', 'cabin', 'embarked', 'boat', 'body', 'home.dest'],\n",
      "      dtype='object')\n",
      "Shape of df:\n",
      "(1309, 2)\n",
      "Shape of df using dropna how=\"any\"\n",
      "(272, 2)\n",
      "Shape of df using dropna how=\"all\"\n",
      "(1069, 2)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1309 entries, 0 to 1308\n",
      "Data columns (total 10 columns):\n",
      "pclass      1309 non-null int64\n",
      "survived    1309 non-null int64\n",
      "name        1309 non-null object\n",
      "sex         1309 non-null object\n",
      "age         1046 non-null float64\n",
      "sibsp       1309 non-null int64\n",
      "parch       1309 non-null int64\n",
      "ticket      1309 non-null object\n",
      "fare        1308 non-null float64\n",
      "embarked    1307 non-null object\n",
      "dtypes: float64(2), int64(4), object(4)\n",
      "memory usage: 102.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "titanic = pd.read_csv('data/titanic.csv')\n",
    "print(titanic.columns)\n",
    "\n",
    "# Select the 'age' and 'cabin' columns: df\n",
    "df = titanic[['age','cabin']]\n",
    "\n",
    "# Print the shape of df\n",
    "print('Shape of df:')\n",
    "print(df.shape)\n",
    "\n",
    "# Drop rows in df with how='any' and print the shape\n",
    "print('Shape of df using dropna how=\"any\"')\n",
    "print(df.dropna(how='any').shape)\n",
    "\n",
    "# Drop rows in df with how='all' and print the shape\n",
    "print('Shape of df using dropna how=\"all\"')\n",
    "print(df.dropna(how='all').shape)\n",
    "\n",
    "# Drop columns in titanic with less than 1000 non-missing values\n",
    "print(titanic.dropna(thresh=1000, axis='columns').info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Using apply() to transform a column</h2>\n",
    "\n",
    "The .apply() method can be used on a pandas DataFrame to apply an arbitrary Python function to every element. In this exercise you'll take daily weather data in Pittsburgh in 2013 obtained from Weather Underground.\n",
    "\n",
    "A function to convert degrees Fahrenheit to degrees Celsius has been written for you. Your job is to use the .apply() method to perform this conversion on the 'Mean TemperatureF' and 'Mean Dew PointF' columns of the weather DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Mean TemperatureC  Mean Dew PointC\n",
      "0          -2.222222        -2.777778\n",
      "1          -6.111111       -11.111111\n",
      "2          -4.444444        -9.444444\n",
      "3          -2.222222        -7.222222\n",
      "4          -1.111111        -6.666667\n"
     ]
    }
   ],
   "source": [
    "weather = pd.read_csv('data/pittsburgh2013.csv')\n",
    "\n",
    "# Write a function to convert degrees Fahrenheit to degrees Celsius: to_celsius\n",
    "def to_celsius(F):\n",
    "    return 5/9*(F - 32)\n",
    "\n",
    "# Apply the function over 'Mean TemperatureF' and 'Mean Dew PointF': df_celsius\n",
    "df_celsius = weather[['Mean TemperatureF','Mean Dew PointF']].apply(to_celsius)\n",
    "\n",
    "# Reassign the columns df_celsius\n",
    "df_celsius.columns = ['Mean TemperatureC', 'Mean Dew PointC']\n",
    "\n",
    "# Print the output of df_celsius.head()\n",
    "print(df_celsius.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Using .map() with a dictionary</h2>\n",
    "\n",
    "The .map() method is used to transform values according to a Python dictionary look-up. In this exercise you'll practice this method while returning to working with the election DataFrame, which has been pre-loaded for you.\n",
    "\n",
    "Your job is to use a dictionary to map the values 'Obama' and 'Romney' in the 'winner' column to the values 'blue' and 'red', and assign the output to the new column 'color'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          state   total      Obama     Romney  winner  voters    turnout  \\\n",
      "county                                                                     \n",
      "Adams        PA   41973  35.482334  63.112001  Romney   61156  68.632677   \n",
      "Allegheny    PA  614671  56.640219  42.185820   Obama  924351  66.497575   \n",
      "Armstrong    PA   28322  30.696985  67.901278  Romney   42147  67.198140   \n",
      "Beaver       PA   80015  46.032619  52.637630  Romney  115157  69.483401   \n",
      "Bedford      PA   21444  22.057452  76.986570  Romney   32189  66.619031   \n",
      "\n",
      "              margin color  \n",
      "county                      \n",
      "Adams      27.629667   red  \n",
      "Allegheny  14.454399  blue  \n",
      "Armstrong  37.204293   red  \n",
      "Beaver      6.605012   red  \n",
      "Bedford    54.929118   red  \n"
     ]
    }
   ],
   "source": [
    "# Create the dictionary: red_vs_blue\n",
    "red_vs_blue = {'Obama':'blue','Romney':'red'}\n",
    "\n",
    "# Use the dictionary to map the 'winner' column to the new column: election['color']\n",
    "election['color'] = election['winner'].map(red_vs_blue)\n",
    "\n",
    "# Print the output of election.head()\n",
    "print(election.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Using vectorized functions</h2>\n",
    "\n",
    "When performance is paramount, you should avoid using .apply() and .map() because those constructs perform Python for-loops over the data stored in a pandas Series or DataFrame. By using vectorized functions instead, you can loop over the data at the same speed as compiled code (C, Fortran, etc.)! NumPy, SciPy and pandas come with a variety of vectorized functions (called Universal Functions or UFuncs in NumPy).\n",
    "\n",
    "You can even write your own vectorized functions, but for now we will focus on the ones distributed by NumPy and pandas.\n",
    "\n",
    "In this exercise you're going to import the zscore function from scipy.stats and use it to compute the deviation in voter turnout in Pennsylvania from the mean in fractions of the standard deviation. In statistics, the z-score is the number of standard deviations by which an observation is above the mean - so if it is negative, it means the observation is below the mean.\n",
    "\n",
    "Instead of using .apply() as you did in the earlier exercises, the zscore UFunc will take a pandas Series as input and return a NumPy array. You will then assign the values of the NumPy array to a new column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "          state   total      Obama     Romney  winner  voters    turnout  \\\n",
      "county                                                                     \n",
      "Adams        PA   41973  35.482334  63.112001  Romney   61156  68.632677   \n",
      "Allegheny    PA  614671  56.640219  42.185820   Obama  924351  66.497575   \n",
      "Armstrong    PA   28322  30.696985  67.901278  Romney   42147  67.198140   \n",
      "Beaver       PA   80015  46.032619  52.637630  Romney  115157  69.483401   \n",
      "Bedford      PA   21444  22.057452  76.986570  Romney   32189  66.619031   \n",
      "\n",
      "              margin color  turnout_zscore  \n",
      "county                                      \n",
      "Adams      27.629667   red        0.853734  \n",
      "Allegheny  14.454399  blue        0.439846  \n",
      "Armstrong  37.204293   red        0.575650  \n",
      "Beaver      6.605012   red        1.018647  \n",
      "Bedford    54.929118   red        0.463391  \n"
     ]
    }
   ],
   "source": [
    "# Import zscore from scipy.stats\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Call zscore with election['turnout'] as input: turnout_zscore\n",
    "turnout_zscore = zscore(election['turnout'])\n",
    "\n",
    "# Print the type of turnout_zscore\n",
    "print(type(turnout_zscore))\n",
    "\n",
    "# Assign turnout_zscore to a new column: election['turnout_zscore']\n",
    "election['turnout_zscore'] = turnout_zscore\n",
    "\n",
    "# Print the output of election.head()\n",
    "print(election.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Chapter 2</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Changing index of a DataFrame</h2>\n",
    "\n",
    "As you saw in the previous exercise, indexes are immutable objects. This means that if you want to change or modify the index in a DataFrame, then you need to change the whole index. You will do this now, using a list comprehension to create the new index.\n",
    "\n",
    "A list comprehension is a succinct way to generate a list in one line. For example, the following list comprehension generates a list that contains the cubes of all numbers from 0 to 9: cubes = [i**3 for i in range(10)]. This is equivalent to the following code:\n",
    "\n",
    "cubes = []\n",
    "for i in range(10):\n",
    "    cubes.append(i**3)\n",
    "Before getting started, print the sales DataFrame in the IPython Shell and verify that the index is given by month abbreviations containing lowercase characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     eggs  salt  spam\n",
      "JAN    47  12.0    17\n",
      "FEB   110  50.0    31\n",
      "MAR   221  89.0    72\n",
      "APR    77  87.0    20\n",
      "MAY   132   NaN    52\n",
      "JUN   205  60.0    55\n"
     ]
    }
   ],
   "source": [
    "sales = pd.read_csv('data/sales.csv', index_col='month')\n",
    "\n",
    "# Create the list of new indexes: new_idx\n",
    "new_idx = [index.upper() for index in sales.index]\n",
    "\n",
    "# Assign new_idx to sales.index\n",
    "sales.index = new_idx\n",
    "\n",
    "# Print the sales DataFrame\n",
    "print(sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Changing index name labels</h2>\n",
    "\n",
    "Notice that in the previous exercise, the index was not labeled with a name. In this exercise, you will set its name to 'MONTHS'.\n",
    "\n",
    "Similarly, if all the columns are related in some way, you can provide a label for the set of columns.\n",
    "\n",
    "To get started, print the sales DataFrame in the IPython Shell and verify that the index has no name, only its data (the month names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        eggs  salt  spam\n",
      "MONTHS                  \n",
      "JAN       47  12.0    17\n",
      "FEB      110  50.0    31\n",
      "MAR      221  89.0    72\n",
      "APR       77  87.0    20\n",
      "MAY      132   NaN    52\n",
      "JUN      205  60.0    55\n",
      "PRODUCTS  eggs  salt  spam\n",
      "MONTHS                    \n",
      "JAN         47  12.0    17\n",
      "FEB        110  50.0    31\n",
      "MAR        221  89.0    72\n",
      "APR         77  87.0    20\n",
      "MAY        132   NaN    52\n",
      "JUN        205  60.0    55\n"
     ]
    }
   ],
   "source": [
    "# Assign the string 'MONTHS' to sales.index.name\n",
    "sales.index.name = 'MONTHS'\n",
    "\n",
    "# Print the sales DataFrame\n",
    "print(sales)\n",
    "\n",
    "# Assign the string 'PRODUCTS' to sales.columns.name \n",
    "sales.columns.name = 'PRODUCTS'\n",
    "\n",
    "# Print the sales dataframe again\n",
    "print(sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Building an index, then a DataFrame</h2>\n",
    "\n",
    "You can also build the DataFrame and index independently, and then put them together. If you take this route, be careful, as any mistakes in generating the DataFrame or the index can cause the data and the index to be aligned incorrectly.\n",
    "\n",
    "In this exercise, the sales DataFrame has been provided for you without the month index. Your job is to build this index separately and then assign it to the sales DataFrame. Before getting started, print the sales DataFrame in the IPython Shell and note that it's missing the month information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRODUCTS  eggs  salt  spam\n",
      "Jan         47  12.0    17\n",
      "Feb        110  50.0    31\n",
      "Mar        221  89.0    72\n",
      "Apr         77  87.0    20\n",
      "May        132   NaN    52\n",
      "Jun        205  60.0    55\n"
     ]
    }
   ],
   "source": [
    "# Generate the list of months: months\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']\n",
    "\n",
    "# Assign months to sales.index\n",
    "sales.index = months\n",
    "\n",
    "# Print the modified sales DataFrame\n",
    "print(sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Extracting data with a MultiIndex</h2>\n",
    "\n",
    "In the video, Dhavide explained the concept of a hierarchical index, or a MultiIndex. You will now practice working with these types of indexes.\n",
    "\n",
    "The sales DataFrame you have been working with has been extended to now include State information as well. In the IPython Shell, print the new sales DataFrame to inspect the data. Take note of the MultiIndex!\n",
    "\n",
    "Extracting elements from the outermost level of a MultiIndex is just like in the case of a single-level Index. You can use the .loc[] accessor as Dhavide demonstrated in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "             eggs  salt  spam\n",
    "state month                  \n",
    "CA    1        47  12.0    17\n",
    "      2       110  50.0    31\n",
    "NY    1       221  89.0    72\n",
    "      2        77  87.0    20\n",
    "TX    1       132   NaN    52\n",
    "      2       205  60.0    55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sales.loc[['CA', 'TX']]\n",
    "print(sales.loc[['CA', 'TX']])\n",
    "\n",
    "# Print sales['CA':'TX']\n",
    "print(sales['CA':'TX'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Setting & sorting a MultiIndex</h2>\n",
    "\n",
    "In the previous exercise, the MultiIndex was created and sorted for you. Now, you're going to do this yourself! With a MultiIndex, you should always ensure the index is sorted. You can skip this only if you know the data is already sorted on the index fields.\n",
    "\n",
    "To get started, print the pre-loaded sales DataFrame in the IPython Shell to verify that there is no MultiIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index to be the columns ['state', 'month']: sales\n",
    "sales = sales.set_index(['state', 'month'])\n",
    "\n",
    "# Sort the MultiIndex: sales\n",
    "sales = sales.sort_index()\n",
    "\n",
    "# Print the sales DataFrame\n",
    "print(sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Using .loc[] with nonunique indexes</h2>\n",
    "\n",
    "As Dhavide mentioned in the video, it is always preferable to have a meaningful index that uniquely identifies each row. Even though pandas does not require unique index values in DataFrames, it works better if the index values are indeed unique. To see an example of this, you will index your sales data by 'state' in this exercise.\n",
    "\n",
    "As always, begin by printing the sales DataFrame in the IPython Shell and inspecting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sales.head())\n",
    "\n",
    "# Set the index to the column 'state': sales\n",
    "sales = sales.set_index(['state'])\n",
    "\n",
    "# Print the sales DataFrame\n",
    "print(sales)\n",
    "\n",
    "# Access the data from 'NY'\n",
    "print(sales.loc['NY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Indexing multiple levels of a MultiIndex</h2>\n",
    "\n",
    "Looking up indexed data is fast and efficient. And you have already seen that lookups based on the outermost level of a MultiIndex work just like lookups on DataFrames that have a single-level Index.\n",
    "\n",
    "Looking up data based on inner levels of a MultiIndex can be a bit trickier. In this exercise, you will use your sales DataFrame to do some increasingly complex lookups.\n",
    "\n",
    "The trickiest of all these lookups are when you want to access some inner levels of the index. In this case, you need to use slice(None) in the slicing parameter for the outermost dimension(s) instead of the usual :, or use pd.IndexSlice. You can refer to the pandas documentation for more details. For example, in the video, Dhavide used the following code to extract rows from all Symbols for the dates Oct. 3rd through 4th inclusive:\n",
    "\n",
    "stocks.loc[(slice(None), slice('2016-10-03', '2016-10-04')), :]\n",
    "\n",
    "Pay particular attention to the tuple (slice(None), slice('2016-10-03', '2016-10-04'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up data for NY in month 1: NY_month1\n",
    "NY_month1 = sales.loc[('NY', 1), :]\n",
    "\n",
    "# Look up data for CA and TX in month 2: CA_TX_month2\n",
    "CA_TX_month2 = sales.loc[(['CA', 'TX'], 2), :]\n",
    "\n",
    "# Look up data for all states in month 2: all_month2\n",
    "all_month2 = sales.loc[(slice(None), 2), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Chapter 3</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pivoting a single variable</h2>\n",
    "Suppose you started a blog for a band, and you would like to log how many visitors you have had, and how many signed-up for your newsletter. To help design the tours later, you track where the visitors are. A DataFrame called users consisting of this information has been pre-loaded for you.\n",
    "\n",
    "Inspect users in the IPython Shell and make a note of which variable you want to use to index the rows ('weekday'), which variable you want to use to index the columns ('city'), and which variable will populate the values in the cells ('visitors'). Try to visualize what the result should be.\n",
    "\n",
    "For example, in the video, Dhavide used 'treatment' to index the rows, 'gender' to index the columns, and 'response' to populate the cells. Prior to pivoting, the DataFrame looked like this:\n",
    "\n",
    "\n",
    "   id treatment gender  response\n",
    "    \n",
    "0   1         A      F         5\n",
    "\n",
    "1   2         A      M         3\n",
    "\n",
    "2   3         B      F         8\n",
    "\n",
    "3   4         B      M         9\n",
    "\n",
    "\n",
    "After pivoting:\n",
    "\n",
    "gender     F  M\n",
    "    \n",
    "treatment      \n",
    "\n",
    "A          5  3\n",
    "\n",
    "B          8  9\n",
    "\n",
    "\n",
    "In this exercise, your job is to pivot users so that the focus is on 'visitors', with the columns indexed by 'city' and the rows indexed by 'weekday'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  weekday    city  visitors  signups\n",
      "0     Sun  Austin       139        7\n",
      "1     Sun  Dallas       237       12\n",
      "2     Mon  Austin       326        3\n",
      "3     Mon  Dallas       456        5\n",
      "city     Austin  Dallas\n",
      "weekday                \n",
      "Mon         326     456\n",
      "Sun         139     237\n"
     ]
    }
   ],
   "source": [
    "users = pd.read_csv('data/users.csv', index_col=0)\n",
    "\n",
    "print(users)\n",
    "\n",
    "# Pivot the users DataFrame: visitors_pivot\n",
    "visitors_pivot = users.pivot(index='weekday', columns='city', values='visitors')\n",
    "\n",
    "# Print the pivoted DataFrame\n",
    "print(visitors_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! Notice how in the pivoted DataFrame, the index is labeled 'weekday', the columns are labeled 'city', and the values are populated by the number of visitors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pivoting all variables</h2>\n",
    "\n",
    "If you do not select any particular variables, all of them will be pivoted. In this case - with the users DataFrame - both 'visitors' and 'signups' will be pivoted, creating hierarchical column labels.\n",
    "\n",
    "You will explore this for yourself now in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city     Austin  Dallas\n",
      "weekday                \n",
      "Mon           3       5\n",
      "Sun           7      12\n",
      "        visitors        signups       \n",
      "city      Austin Dallas  Austin Dallas\n",
      "weekday                               \n",
      "Mon          326    456       3      5\n",
      "Sun          139    237       7     12\n"
     ]
    }
   ],
   "source": [
    "# Pivot users with signups indexed by weekday and city: signups_pivot\n",
    "signups_pivot = users.pivot(index='weekday', columns='city', values='signups')\n",
    "\n",
    "# Print signups_pivot\n",
    "print(signups_pivot)\n",
    "\n",
    "# Pivot users pivoted by both signups and visitors: pivot\n",
    "pivot = users.pivot(index='weekday', columns='city')\n",
    "\n",
    "# Print the pivoted DataFrame\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! Notice how in the second DataFrame, both 'signups' and 'visitors' were pivoted by default since you didn't provide an argument for the values parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Stacking & unstacking I</h2>\n",
    "\n",
    "You are now going to practice stacking and unstacking DataFrames. The users DataFrame you have been working with in this chapter has been pre-loaded for you, this time with a MultiIndex. Explore it in the IPython Shell to see the data layout. Pay attention to the index, and notice that the index levels are ['city', 'weekday']. So 'weekday' - the second entry - has position 1. This position is what corresponds to the level parameter in .stack() and .unstack() calls. Alternatively, you can specify 'weekday' as the level instead of its position.\n",
    "\n",
    "Your job in this exercise is to unstack users by 'weekday'. You will then use .stack() on the unstacked DataFrame to see if you get back the original layout of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weekday   0       Sun\n",
      "          1       Sun\n",
      "          2       Mon\n",
      "          3       Mon\n",
      "city      0    Austin\n",
      "          1    Dallas\n",
      "          2    Austin\n",
      "          3    Dallas\n",
      "visitors  0       139\n",
      "          1       237\n",
      "          2       326\n",
      "          3       456\n",
      "signups   0         7\n",
      "          1        12\n",
      "          2         3\n",
      "          3         5\n",
      "dtype: object\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'stack'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-4e5212429bb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Stack byweekday by 'weekday' and print it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyweekday\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weekday'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\keras\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   4374\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4375\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4376\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4378\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'stack'"
     ]
    }
   ],
   "source": [
    "# Unstack users by 'weekday': byweekday\n",
    "byweekday = users.unstack(level='weekday')\n",
    "\n",
    "# Print the byweekday DataFrame\n",
    "print(byweekday)\n",
    "\n",
    "# Stack byweekday by 'weekday' and print it\n",
    "print(byweekday.stack(level='weekday'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Stacking & unstacking II</h2>\n",
    "\n",
    "You are now going to continue working with the users DataFrame. As always, first explore it in the IPython Shell to see the layout and note the index.\n",
    "\n",
    "Your job in this exercise is to unstack and then stack the 'city' level, as you did previously for 'weekday'. Note that you won't get the same DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weekday   0       Sun\n",
      "          1       Sun\n",
      "          2       Mon\n",
      "          3       Mon\n",
      "city      0    Austin\n",
      "          1    Dallas\n",
      "          2    Austin\n",
      "          3    Dallas\n",
      "visitors  0       139\n",
      "          1       237\n",
      "          2       326\n",
      "          3       456\n",
      "signups   0         7\n",
      "          1        12\n",
      "          2         3\n",
      "          3         5\n",
      "dtype: object\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'stack'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-b8973fdaa8c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Stack bycity by 'city' and print it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbycity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'city'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\keras\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   4374\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4375\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4376\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4378\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'stack'"
     ]
    }
   ],
   "source": [
    "# Unstack users by 'city': bycity\n",
    "bycity = users.unstack(level='city')\n",
    "\n",
    "# Print the bycity DataFrame\n",
    "print(bycity)\n",
    "\n",
    "# Stack bycity by 'city' and print it\n",
    "print(bycity.stack(level='city'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Restoring the index order</h2>\n",
    "\n",
    "Continuing from the previous exercise, you will now use .swaplevel(0, 1) to flip the index levels. Note they won't be sorted. To sort them, you will have to follow up with a .sort_index(). You will then obtain the original DataFrame. Note that an unsorted index leads to slicing failures.\n",
    "\n",
    "To begin, print both users and bycity in the IPython Shell. The goal here is to convert bycity back to something that looks like users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'stack'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-dfe56840d58d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Stack 'city' back into the index of bycity: newusers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnewusers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbycity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'city'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Swap the levels of the index of newusers: newusers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnewusers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnewusers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswaplevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\keras\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   4374\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4375\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4376\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4378\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'stack'"
     ]
    }
   ],
   "source": [
    "# Stack 'city' back into the index of bycity: newusers\n",
    "newusers = bycity.stack(level='city')\n",
    "\n",
    "# Swap the levels of the index of newusers: newusers\n",
    "newusers = newusers.swaplevel(0, 1)\n",
    "\n",
    "# Print newusers and verify that the index is not sorted\n",
    "print(newusers)\n",
    "\n",
    "# Sort the index of newusers: newusers\n",
    "newusers = newusers.sort_index()\n",
    "\n",
    "# Print newusers and verify that the index is now sorted\n",
    "print(newusers)\n",
    "\n",
    "# Verify that the new DataFrame is equal to the original\n",
    "print(newusers.equals(users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Adding names for readability</h2>\n",
    "\n",
    "You are now going to practice melting DataFrames. A DataFrame called visitors_by_city_weekday has been pre-loaded for you. Explore it in the IPython Shell and see that it is the users DataFrame from previous exercises with the rows indexed by 'weekday', columns indexed by 'city', and values populated with 'visitors'.\n",
    "\n",
    "Recall from the video that the goal of melting is to restore a pivoted DataFrame to its original form, or to change it from a wide shape to a long shape. You can explicitly specify the columns that should remain in the reshaped DataFrame with id_vars, and list which columns to convert into values with value_vars. As Dhavide demonstrated, if you don't pass a name to the values in pd.melt(), you will lose the name of your variable. You can fix this by using the value_name keyword argument.\n",
    "\n",
    "Your job in this exercise is to melt visitors_by_city_weekday to move the city names from the column labels to values in a single column called 'city'. If you were to use just pd.melt(visitors_by_city_weekday), you would obtain the following result:\n",
    "\n",
    "      city value\n",
    "      \n",
    "0  weekday   Mon\n",
    "\n",
    "1  weekday   Sun\n",
    "\n",
    "2   Austin   326\n",
    "\n",
    "3   Austin   139\n",
    "\n",
    "4   Dallas   456\n",
    "\n",
    "5   Dallas   237\n",
    "\n",
    "\n",
    "Therefore, you have to specify the id_vars keyword argument to ensure that 'weekday' is retained in the reshaped DataFrame, and the value_name keyword argument to change the name of value to visitors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'visitors_by_city_weekday' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-619169c56c11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Reset the index: visitors_by_city_weekday\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvisitors_by_city_weekday\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvisitors_by_city_weekday\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Print visitors_by_city_weekday\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvisitors_by_city_weekday\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'visitors_by_city_weekday' is not defined"
     ]
    }
   ],
   "source": [
    "# Reset the index: visitors_by_city_weekday\n",
    "visitors_by_city_weekday = visitors_by_city_weekday.reset_index() \n",
    "\n",
    "# Print visitors_by_city_weekday\n",
    "print(visitors_by_city_weekday)\n",
    "\n",
    "# Melt visitors_by_city_weekday: visitors\n",
    "visitors = pd.melt(visitors_by_city_weekday, id_vars=['weekday'], value_name='visitors')\n",
    "\n",
    "# Print visitors\n",
    "print(visitors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! Notice how your melted DataFrame now has a 'city' column with Austin and Dallas as its values. In the original DataFrame, they were columns themselves. Also note how specifying the value_name parameter has renamed the 'value' column to 'visitors'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Going from wide to long</h2>\n",
    "You can move multiple columns into a single column (making the data long and skinny) by \"melting\" multiple columns. In this exercise, you will practice doing this.\n",
    "\n",
    "The users DataFrame has been pre-loaded for you. As always, explore it in the IPython Shell and note the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  weekday    city  variable  value\n",
      "0     Sun  Austin  visitors    139\n",
      "1     Sun  Dallas  visitors    237\n",
      "2     Mon  Austin  visitors    326\n",
      "3     Mon  Dallas  visitors    456\n",
      "4     Sun  Austin   signups      7\n",
      "5     Sun  Dallas   signups     12\n",
      "6     Mon  Austin   signups      3\n",
      "7     Mon  Dallas   signups      5\n"
     ]
    }
   ],
   "source": [
    "# Melt users: skinny\n",
    "skinny = pd.melt(users, id_vars=['weekday' ,'city'])\n",
    "\n",
    "# Print skinny\n",
    "print(skinny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! Here, because you didn't specify the var_name or value_name parameters, the melted DataFrame has the default variable and value column names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Obtaining key-value pairs with melt()</h2>\n",
    "\n",
    "Sometimes, all you need is some key-value pairs, and the context does not matter. If said context is in the index, you can easily obtain what you want. For example, in the users DataFrame, the visitors and signups columns lend themselves well to being represented as key-value pairs. So if you created a hierarchical index with 'city' and 'weekday' columns as the index, you can easily extract key-value pairs for the 'visitors' and 'signups' columns by melting users and specifying col_level=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                visitors  signups\n",
      "city   weekday                   \n",
      "Austin Sun           139        7\n",
      "Dallas Sun           237       12\n",
      "Austin Mon           326        3\n",
      "Dallas Mon           456        5\n",
      "   variable  value\n",
      "0  visitors    139\n",
      "1  visitors    237\n",
      "2  visitors    326\n",
      "3  visitors    456\n",
      "4   signups      7\n",
      "5   signups     12\n",
      "6   signups      3\n",
      "7   signups      5\n"
     ]
    }
   ],
   "source": [
    "# Set the new index: users_idx\n",
    "users_idx = users.set_index(['city', 'weekday'])\n",
    "\n",
    "# Print the users_idx DataFrame\n",
    "print(users_idx)\n",
    "\n",
    "# Obtain the key-value pairs: kv_pairs\n",
    "kv_pairs = pd.melt(users_idx, col_level=0)\n",
    "\n",
    "# Print the key-value pairs\n",
    "print(kv_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! It's always worth keeping in mind whether any aspects of your data lend themselves well to being represented as key-value pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Setting up a pivot table</h2>\n",
    "\n",
    "Recall from the video that a pivot table allows you to see all of your variables as a function of two other variables. In this exercise, you will use the .pivot_table() method to see how the users DataFrame entries appear when presented as functions of the 'weekday' and 'city' columns. That is, with the rows indexed by 'weekday' and the columns indexed by 'city'.\n",
    "\n",
    "Before using the pivot table, print the users DataFrame in the IPython Shell and observe the layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        signups        visitors       \n",
      "city     Austin Dallas   Austin Dallas\n",
      "weekday                               \n",
      "Mon           3      5      326    456\n",
      "Sun           7     12      139    237\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame with the appropriate pivot table: by_city_day\n",
    "by_city_day = users.pivot_table(index='weekday', columns='city')\n",
    "\n",
    "# Print by_city_day\n",
    "print(by_city_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! Notice the labels of the index and the columns are 'weekday' and 'city', respectively - exactly as you specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Using other aggregations in pivot tables</h2>\n",
    "\n",
    "You can also use aggregation functions within a pivot table by specifying the aggfunc parameter. In this exercise, you will practice using the 'count' and len aggregation functions - which produce the same result - on the users DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         city  signups  visitors\n",
      "weekday                         \n",
      "Mon         2        2         2\n",
      "Sun         2        2         2\n",
      "==========================================\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Use a pivot table to display the count of each column: count_by_weekday1\n",
    "count_by_weekday1 = users.pivot_table(index='weekday', aggfunc='count')\n",
    "\n",
    "# Print count_by_weekday\n",
    "print(count_by_weekday1)\n",
    "\n",
    "# Replace 'aggfunc='count'' with 'aggfunc=len': count_by_weekday2\n",
    "count_by_weekday2 = users.pivot_table(index='weekday', aggfunc=len)\n",
    "\n",
    "# Verify that the same result is obtained\n",
    "print('==========================================')\n",
    "print(count_by_weekday1.equals(count_by_weekday2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! As expected, both the len and 'count' aggregation functions produced the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Using margins in pivot tables</h2>\n",
    "\n",
    "Sometimes it's useful to add totals in the margins of a pivot table. You can do this with the argument margins=True. In this exercise, you will practice using margins in a pivot table along with a new aggregation function: sum.\n",
    "\n",
    "The users DataFrame, which you are now probably very familiar with, has been pre-loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         signups  visitors\n",
      "weekday                   \n",
      "Mon            8       782\n",
      "Sun           19       376\n",
      "         signups  visitors\n",
      "weekday                   \n",
      "Mon            8       782\n",
      "Sun           19       376\n",
      "All           27      1158\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame with the appropriate pivot table: signups_and_visitors\n",
    "signups_and_visitors = users.pivot_table(index='weekday', aggfunc=sum)\n",
    "\n",
    "# Print signups_and_visitors\n",
    "print(signups_and_visitors)\n",
    "\n",
    "# Add in the margins: signups_and_visitors_total \n",
    "signups_and_visitors_total = users.pivot_table(index='weekday', aggfunc=sum, margins=True)\n",
    "\n",
    "# Print signups_and_visitors_total\n",
    "print(signups_and_visitors_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! Take a look at how specifying margins=True resulted in the totals in each column being computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Chapter 4</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Grouping by multiple columns</h2>\n",
    "\n",
    "In this exercise, you will return to working with the Titanic dataset from Chapter 1 and use .groupby() to analyze the distribution of passengers who boarded the Titanic.\n",
    "\n",
    "The 'pclass' column identifies which class of ticket was purchased by the passenger and the 'embarked' column indicates at which of the three ports the passenger boarded the Titanic. 'S' stands for Southampton, England, 'C' for Cherbourg, France and 'Q' for Queenstown, Ireland.\n",
    "\n",
    "Your job is to first group by the 'pclass' column and count the number of rows in each class using the 'survived' column. You will then group by the 'embarked' and 'pclass' columns and count the number of passengers.\n",
    "\n",
    "The DataFrame has been pre-loaded as titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group titanic by 'pclass'\n",
    "by_class = titanic.groupby('pclass')\n",
    "\n",
    "# Aggregate 'survived' column of by_class by count\n",
    "count_by_class = by_class['survived'].count()\n",
    "\n",
    "# Print count_by_class\n",
    "print(count_by_class)\n",
    "\n",
    "# Group titanic by 'embarked' and 'pclass'\n",
    "by_mult = titanic.groupby(['embarked','pclass'])\n",
    "\n",
    "# Aggregate 'survived' column of by_mult by count\n",
    "count_mult = by_mult['survived'].count()\n",
    "\n",
    "# Print count_mult\n",
    "print(count_mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! Grouping your data by certain columns like this and aggregating them by another column - in this case, 'survived' - allows you to carefully examine your data for interesting insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Grouping by another series</h2>\n",
    "\n",
    "In this exercise, you'll use two data sets from Gapminder.org to investigate the average life expectancy (in years) at birth in 2010 for the 6 continental regions. To do this you'll read the life expectancy data per country into one pandas DataFrame and the association between country and region into another.\n",
    "\n",
    "By setting the index of both DataFrames to the country name, you'll then use the region information to group the countries in the life expectancy DataFrame and compute the mean value for 2010.\n",
    "\n",
    "The life expectancy CSV file is available to you in the variable life_fname and the regions filename is available in the variable regions_fname."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read life_fname into a DataFrame: life\n",
    "life = pd.read_csv(life_fname, index_col='Country')\n",
    "\n",
    "# Read regions_fname into a DataFrame: regions\n",
    "regions = pd.read_csv(regions_fname, index_col='Country')\n",
    "\n",
    "# Group life by regions['region']: life_by_region\n",
    "life_by_region = life.groupby(regions['region'])\n",
    "\n",
    "# Print the mean over the '2010' column of life_by_region\n",
    "print(life_by_region['2010'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! It looks like the average life expectancy (in years) at birth in 2010 was highest in Europe & Central Asia and lowest in Sub-Saharan Africa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Computing multiple aggregates of multiple columns</h2>\n",
    "\n",
    "The .agg() method can be used with a tuple or list of aggregations as input. When applying multiple aggregations on multiple columns, the aggregated DataFrame has a multi-level column index.\n",
    "\n",
    "In this exercise, you're going to group passengers on the Titanic by 'pclass' and aggregate the 'age' and 'fare' columns by the functions 'max' and 'median'. You'll then use multi-level selection to find the oldest passenger per class and the median fare price per class.\n",
    "\n",
    "The DataFrame has been pre-loaded as titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group titanic by 'pclass': by_class\n",
    "by_class = titanic.groupby('pclass')\n",
    "\n",
    "# Select 'age' and 'fare'\n",
    "by_class_sub = by_class[['age','fare']]\n",
    "\n",
    "# Aggregate by_class_sub by 'max' and 'median': aggregated\n",
    "aggregated = by_class_sub.agg(['max','median'])\n",
    "\n",
    "# Print the maximum age in each class\n",
    "print(aggregated.loc[:, ('age','max')])\n",
    "\n",
    "# Print the median fare in each class\n",
    "print(aggregated.loc[:, ('fare','median')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic work! It isn't surprising that the highest median fare was for the 1st passenger class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Aggregating on index levels/fields</h2>\n",
    "\n",
    "If you have a DataFrame with a multi-level row index, the individual levels can be used to perform the groupby. This allows advanced aggregation techniques to be applied along one or more levels in the index and across one or more columns.\n",
    "\n",
    "In this exercise you'll use the full Gapminder dataset which contains yearly values of life expectancy, population, child mortality (per 1,000) and per capita gross domestic product (GDP) for every country in the world from 1964 to 2013.\n",
    "\n",
    "Your job is to create a multi-level DataFrame of the columns 'Year', 'Region' and 'Country'. Next you'll group the DataFrame by the 'Year' and 'Region' levels. Finally, you'll apply a dictionary aggregation to compute the total population, spread of per capita GDP values and average child mortality rate.\n",
    "\n",
    "The Gapminder CSV file is available as 'gapminder.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame and sort the index: gapminder\n",
    "gapminder = pd.read_csv('gapminder.csv', index_col=['Year','region','Country']).sort_index()\n",
    "\n",
    "# Group gapminder by 'Year' and 'region': by_year_region\n",
    "by_year_region = gapminder.groupby(level=['Year','region'])\n",
    "\n",
    "# Define the function to compute spread: spread\n",
    "def spread(series):\n",
    "    return series.max() - series.min()\n",
    "\n",
    "# Create the dictionary: aggregator\n",
    "aggregator = {'population':'sum', 'child_mortality':'mean', 'gdp':spread}\n",
    "\n",
    "# Aggregate by_year_region using the dictionary: aggregated\n",
    "aggregated = by_year_region.agg(aggregator)\n",
    "\n",
    "# Print the last 6 entries of aggregated \n",
    "print(aggregated.tail(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent work! Are you able to see any correlations between population, child_mortality, and gdp?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Grouping on a function of the index</h2>\n",
    "\n",
    "Groupby operations can also be performed on transformations of the index values. In the case of a DateTimeIndex, we can extract portions of the datetime over which to group.\n",
    "\n",
    "In this exercise you'll read in a set of sample sales data from February 2015 and assign the 'Date' column as the index. Your job is to group the sales data by the day of the week and aggregate the sum of the 'Units' column.\n",
    "\n",
    "Is there a day of the week that is more popular for customers? To find out, you're going to use .strftime('%a') to transform the index datetime values to abbreviated days of the week.\n",
    "\n",
    "The sales data CSV file is available to you as 'sales.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file: sales\n",
    "sales = pd.read_csv('sales.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Create a groupby object: by_day\n",
    "by_day = sales.groupby(sales.index.strftime('%a'))\n",
    "\n",
    "# Create sum: units_sum\n",
    "units_sum = by_day['Units'].sum()\n",
    "\n",
    "# Print units_sum\n",
    "print(units_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! It looks like Monday, Wednesday, and Thursday were the most popular days for customers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Detecting outliers with Z-Scores</h2>\n",
    "\n",
    "As Dhavide demonstrated in the video using the zscore function, you can apply a .transform() method after grouping to apply a function to groups of data independently. The z-score is also useful to find outliers: a z-score value of +/- 3 is generally considered to be an outlier.\n",
    "\n",
    "In this example, you're going to normalize the Gapminder data in 2010 for life expectancy and fertility by the z-score per region. Using boolean indexing, you will filter out countries that have high fertility rates and low life expectancy for their region.\n",
    "\n",
    "The Gapminder DataFrame for 2010 indexed by 'Country' is provided for you as gapminder_2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import zscore\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Group gapminder_2010: standardized\n",
    "standardized = gapminder_2010.groupby('region')['life','fertility'].transform(zscore)\n",
    "\n",
    "# Construct a Boolean Series to identify outliers: outliers\n",
    "outliers = (standardized['life'] < -3) | (standardized['fertility'] > 3)\n",
    "\n",
    "# Filter gapminder_2010 by the outliers: gm_outliers\n",
    "gm_outliers = gapminder_2010.loc[outliers]\n",
    "\n",
    "# Print gm_outliers\n",
    "print(gm_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful work! Using z-scores like this is a great way to identify outliers in your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Filling missing data (imputation) by group</h2>\n",
    "\n",
    "Many statistical and machine learning packages cannot determine the best action to take when missing data entries are encountered. Dealing with missing data is natural in pandas (both in using the default behavior and in defining a custom behavior). In Chapter 1, you practiced using the .dropna() method to drop missing values. Now, you will practice imputing missing values. You can use .groupby() and .transform() to fill missing data appropriately for each group.\n",
    "\n",
    "Your job is to fill in missing 'age' values for passengers on the Titanic with the median age from their 'gender' and 'pclass'. To do this, you'll group by the 'sex' and 'pclass' columns and transform each group with a custom function to call .fillna() and impute the median value.\n",
    "\n",
    "The DataFrame has been pre-loaded as titanic. Explore it in the IPython Shell by printing the output of titanic.tail(10). Notice in particular the NaNs in the 'age' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a groupby object: by_sex_class\n",
    "by_sex_class = titanic.groupby(['sex','pclass'])\n",
    "\n",
    "# Write a function that imputes median\n",
    "def impute_median(series):\n",
    "    return series.fillna(series.median())\n",
    "\n",
    "# Impute age and assign to titanic.age\n",
    "titanic.age = by_sex_class.age.transform(impute_median)\n",
    "\n",
    "# Print the output of titanic.tail(10)\n",
    "print(titanic.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! Imputing missing values intelligently is always preferrable to dropping them entirely!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Other transformations with .apply</h2>\n",
    "\n",
    "The .apply() method when used on a groupby object performs an arbitrary function on each of the groups. These functions can be aggregations, transformations or more complex workflows. The .apply() method will then combine the results in an intelligent way.\n",
    "\n",
    "In this exercise, you're going to analyze economic disparity within regions of the world using the Gapminder data set for 2010. To do this you'll define a function to compute the aggregate spread of per capita GDP in each region and the individual country's z-score of the regional per capita GDP. You'll then select three countries - United States, Great Britain and China - to see a summary of the regional GDP and that country's z-score against the regional mean.\n",
    "\n",
    "The 2010 Gapminder DataFrame is provided for you as gapminder_2010. Pandas has been imported as pd.\n",
    "\n",
    "The following function has been defined for your use:\n",
    "\n",
    "def disparity(gr):\n",
    "    # Compute the spread of gr['gdp']: s\n",
    "    s = gr['gdp'].max() - gr['gdp'].min()\n",
    "    # Compute the z-score of gr['gdp'] as (gr['gdp']-gr['gdp'].mean())/gr['gdp'].std(): z\n",
    "    z = (gr['gdp'] - gr['gdp'].mean())/gr['gdp'].std()\n",
    "    # Return a DataFrame with the inputs {'z(gdp)':z, 'regional spread(gdp)':s}\n",
    "    return pd.DataFrame({'z(gdp)':z , 'regional spread(gdp)':s})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group gapminder_2010 by 'region': regional\n",
    "regional = gapminder_2010.groupby('region')\n",
    "\n",
    "# Apply the disparity function on regional: reg_disp\n",
    "reg_disp = regional.apply(disparity)\n",
    "\n",
    "# Print the disparity of 'United States', 'United Kingdom', and 'China'\n",
    "print(reg_disp.loc[['United States','United Kingdom','China']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Grouping and filtering with .apply()</h2>\n",
    "\n",
    "By using .apply(), you can write functions that filter rows within groups. The .apply() method will handle the iteration over individual groups and then re-combine them back into a Series or DataFrame.\n",
    "\n",
    "In this exercise you'll take the Titanic data set and analyze survival rates from the 'C' deck, which contained the most passengers. To do this you'll group the dataset by 'sex' and then use the .apply() method on a provided user defined function which calculates the mean survival rates on the 'C' deck:\n",
    "\n",
    "def c_deck_survival(gr):\n",
    "\n",
    "    c_passengers = gr['cabin'].str.startswith('C').fillna(False)\n",
    "\n",
    "    return gr.loc[c_passengers, 'survived'].mean()\n",
    "    \n",
    "The DataFrame has been pre-loaded as titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a groupby object using titanic over the 'sex' column: by_sex\n",
    "by_sex = titanic.groupby('sex')\n",
    "\n",
    "# Call by_sex.apply with the function c_deck_survival\n",
    "c_surv_by_sex = by_sex.apply(c_deck_survival)\n",
    "\n",
    "# Print the survival rates\n",
    "print(c_surv_by_sex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent work! It looks like female passengers on the 'C' deck had a much higher chance of surviving!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Grouping and filtering with .filter()</h2>\n",
    "\n",
    "You can use groupby with the .filter() method to remove whole groups of rows from a DataFrame based on a boolean condition.\n",
    "\n",
    "In this exercise, you'll take the February sales data and remove entries from companies that purchased less than or equal to 35 Units in the whole month.\n",
    "\n",
    "First, you'll identify how many units each company bought for verification. Next you'll use the .filter() method after grouping by 'Company' to remove all rows belonging to companies whose sum over the 'Units' column was less than or equal to 35. Finally, verify that the three companies whose total Units purchased were less than or equal to 35 have been filtered out from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame: sales\n",
    "sales = pd.read_csv('sales.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Group sales by 'Company': by_company\n",
    "by_company = sales.groupby('Company')\n",
    "\n",
    "# Compute the sum of the 'Units' of by_company: by_com_sum\n",
    "by_com_sum = by_company['Units'].sum()\n",
    "print(by_com_sum)\n",
    "\n",
    "# Filter 'Units' where the sum is > 35: by_com_filt\n",
    "by_com_filt = by_company.filter(lambda g:g['Units'].sum() > 35)\n",
    "print(by_com_filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Filtering and grouping with .map()</h2>\n",
    "\n",
    "You have seen how to group by a column, or by multiple columns. Sometimes, you may instead want to group by a function/transformation of a column. The key here is that the Series is indexed the same way as the DataFrame. You can also mix and match column grouping with Series grouping.\n",
    "\n",
    "In this exercise your job is to investigate survival rates of passengers on the Titanic by 'age' and 'pclass'. In particular, the goal is to find out what fraction of children under 10 survived in each 'pclass'. You'll do this by first creating a boolean array where True is passengers under 10 years old and False is passengers over 10. You'll use .map() to change these values to strings.\n",
    "\n",
    "Finally, you'll group by the under 10 series and the 'pclass' column and aggregate the 'survived' column. The 'survived' column has the value 1 if the passenger survived and 0 otherwise. The mean of the 'survived' column is the fraction of passengers who lived.\n",
    "\n",
    "The DataFrame has been pre-loaded for you as titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Boolean Series: under10\n",
    "under10 = (titanic['age'] < 10).map({True:'under 10', False:'over 10'})\n",
    "\n",
    "# Group by under10 and compute the survival rate\n",
    "survived_mean_1 = titanic.groupby(under10)['survived'].mean()\n",
    "print(survived_mean_1)\n",
    "\n",
    "# Group by under10 and pclass and compute the survival rate\n",
    "survived_mean_2 = titanic.groupby([under10, 'pclass'])['survived'].mean()\n",
    "print(survived_mean_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent work! It looks like passengers under the age of 10 had a higher survival rate than those above the age of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Chapter 5 - Case Study</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Using .value_counts() for ranking</h2>\n",
    "For this exercise, you will use the pandas Series method .value_counts() to determine the top 15 countries ranked by total number of medals.\n",
    "\n",
    "Notice that .value_counts() sorts by values by default. The result is returned as a Series of counts indexed by unique entries from the original Series with values (counts) ranked in descending order.\n",
    "\n",
    "The DataFrame has been pre-loaded for you as medals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 'NOC' column of medals: country_names\n",
    "country_names = medals['NOC']\n",
    "\n",
    "# Count the number of medals won by each country: medal_counts\n",
    "medal_counts = country_names.value_counts()\n",
    "\n",
    "# Print top 15 countries ranked by medals\n",
    "print(medal_counts.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! It looks like the top 5 countries here are USA, URS, GBR, FRA, and ITA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Using .pivot_table() to count medals by type</h2>\n",
    "\n",
    "Rather than ranking countries by total medals won and showing that list, you may want to see a bit more detail. You can use a pivot table to compute how many separate bronze, silver and gold medals each country won. That pivot table can then be used to repeat the previous computation to rank by total medals won.\n",
    "\n",
    "In this exercise, you will use .pivot_table() first to aggregate the total medals by type. Then, you can use .sum() along the columns of the pivot table to produce a new column. When the modified pivot table is sorted by the total medals column, you can display the results from the last exercise with a bit more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the pivot table: counted\n",
    "counted = medals.pivot_table(index='NOC', values='Athlete', columns='Medal', aggfunc='count')\n",
    "\n",
    "# Create the new column: counted['totals']\n",
    "counted['totals'] = counted.sum(axis='columns')\n",
    "\n",
    "# Sort counted by the 'totals' column\n",
    "counted = counted.sort_values('totals', ascending=False)\n",
    "\n",
    "# Print the top 15 rows of counted\n",
    "print(counted.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic work! Take a moment to look at the results and see if you find anything interesting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Applying .drop_duplicates()</h2>\n",
    "\n",
    "What could be the difference between the 'Event_gender' and 'Gender' columns? You should be able to evaluate your guess by looking at the unique values of the pairs (Event_gender, Gender) in the data. In particular, you should not see something like (Event_gender='M', Gender='Women'). However, you will see that, strangely enough, there is an observation with (Event_gender='W', Gender='Men').\n",
    "\n",
    "The duplicates can be dropped using the .drop_duplicates() method, leaving behind the unique observations. The DataFrame has been loaded as medals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns: ev_gen\n",
    "ev_gen = medals[['Event_gender', 'Gender']]\n",
    "\n",
    "# Drop duplicate pairs: ev_gen_uniques\n",
    "ev_gen_uniques = ev_gen.drop_duplicates()\n",
    "\n",
    "# Print ev_gen_uniques\n",
    "print(ev_gen_uniques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Finding possible errors with .groupby()</h2>\n",
    "\n",
    "You will now use .groupby() to continue your exploration. Your job is to group by 'Event_gender' and 'Gender' and count the rows.\n",
    "\n",
    "You will see that there is only one suspicious row: This is likely a data error.\n",
    "\n",
    "The DataFrame is available to you as medals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group medals by the two columns: medals_by_gender\n",
    "medals_by_gender = medals.groupby(['Event_gender', 'Gender'])\n",
    "\n",
    "# Create a DataFrame with a group count: medal_count_by_gender\n",
    "medal_count_by_gender = medals_by_gender.count()\n",
    "\n",
    "# Print medal_count_by_gender\n",
    "print(medal_count_by_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! You're close to identifying the suspicious data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Locating suspicious data</h2>\n",
    "\n",
    "You will now inspect the suspect record by locating the offending row.\n",
    "\n",
    "You will see that, according to the data, Joyce Chepchumba was a man that won a medal in a women's event. That is a data error as you can confirm with a web search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Boolean Series: sus\n",
    "sus = (medals.Event_gender == 'W') & (medals.Gender == 'Men')\n",
    "\n",
    "# Create a DataFrame with the suspicious row: suspect\n",
    "suspect = medals[sus] \n",
    "\n",
    "# Print suspect\n",
    "print(suspect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! You have to always watch out for errors like this in your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Using .nunique() to rank by distinct sports</h2>\n",
    "\n",
    "You may want to know which countries won medals in the most distinct sports. The .nunique() method is the principal aggregation here. Given a categorical Series S, S.nunique() returns the number of distinct categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group medals by 'NOC': country_grouped\n",
    "country_grouped = medals.groupby('NOC')\n",
    "\n",
    "# Compute the number of distinct sports in which each country won medals: Nsports\n",
    "Nsports = country_grouped['Sport'].nunique()\n",
    "\n",
    "# Sort the values of Nsports in descending order\n",
    "Nsports = Nsports.sort_values(ascending=False)\n",
    "\n",
    "# Print the top 15 rows of Nsports\n",
    "print(Nsports.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! Interestingly, the USSR is not in the top 5 in this category, while the USA continues to remain on top. What could be the cause of this? You'll compare the medal counts of USA vs. USSR more closely in the next two exercises to find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Counting USA vs. USSR Cold War Olympic Sports</h2>\n",
    "\n",
    "The Olympic competitions between 1952 and 1988 took place during the height of the Cold War between the United States of America (USA) & the Union of Soviet Socialist Republics (USSR). Your goal in this exercise is to aggregate the number of distinct sports in which the USA and the USSR won medals during the Cold War years.\n",
    "\n",
    "The construction is mostly the same as in the preceding exercise. There is an additional filtering stage beforehand in which you reduce the original DataFrame medals by extracting data from the Cold War period that applies only to the US or to the USSR. The relevant country codes in the DataFrame, which has been pre-loaded as medals, are 'USA' & 'URS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all rows for which the 'Edition' is between 1952 & 1988: during_cold_war\n",
    "during_cold_war = (medals.Edition>=1952) & (medals.Edition<=1988)\n",
    "\n",
    "# Extract rows for which 'NOC' is either 'USA' or 'URS': is_usa_urs\n",
    "is_usa_urs = medals.NOC.isin(['USA', 'URS'])\n",
    "\n",
    "# Use during_cold_war and is_usa_urs to create the DataFrame: cold_war_medals\n",
    "cold_war_medals = medals.loc[during_cold_war & is_usa_urs]\n",
    "\n",
    "# Group cold_war_medals by 'NOC'\n",
    "country_grouped = cold_war_medals.groupby('NOC')\n",
    "\n",
    "# Create Nsports\n",
    "Nsports = country_grouped['Sport'].nunique().sort_values(ascending=False)\n",
    "\n",
    "# Print Nsports\n",
    "print(Nsports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! As you can see, the USSR is actually higher than the US when you look only at the Olympic competitions between 1952 and 1988!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Counting USA vs. USSR Cold War Olympic Medals</h2>\n",
    "\n",
    "For this exercise, you want to see which country, the USA or the USSR, won the most medals consistently over the Cold War period.\n",
    "\n",
    "There are several steps involved in carrying out this computation.\n",
    "\n",
    "<ul><li>You'll need a pivot table with years ('Edition') on the index and countries ('NOC') on the columns. The entries will be the total number of medals each country won that year. If the country won no medals in a given edition, expect a NaN in that entry of the pivot table.</li>\n",
    "<li>You'll need to slice the Cold War period and subset the 'USA' and 'URS' columns.</li>\n",
    "<li>You'll need to make a Series from this slice of the pivot table that tells which country won the most medals in that edition using .idxmax(axis='columns'). If .max() returns the maximum value of Series or 1D array, .idxmax() returns the index of the maximizing element. The argument axis=columns or axis=1 is required because, by default, this aggregation would be done along columns for a DataFrame.</li>\n",
    "<li>The final Series contains either 'USA' or 'URS' according to which country won the most medals in each Olympic edition. You can use .value_counts() to count the number of occurrences of each.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pivot table: medals_won_by_country\n",
    "medals_won_by_country = medals.pivot_table(index='Edition', columns='NOC', values='Athlete', aggfunc='count')\n",
    "\n",
    "# Slice medals_won_by_country: cold_war_usa_urs_medals\n",
    "cold_war_usa_urs_medals = medals_won_by_country.loc[1952:1988, ['USA','URS']]\n",
    "\n",
    "# Create most_medals\n",
    "most_medals = cold_war_usa_urs_medals.idxmax(axis='columns')\n",
    "\n",
    "# Print most_medals.value_counts()\n",
    "print(most_medals.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! Here, once again, the USSR comes out on top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Visualizing USA Medal Counts by Edition: Line Plot</h2>\n",
    "\n",
    "Your job in this exercise is to visualize the medal counts by 'Edition' for the USA. The DataFrame has been pre-loaded for you as medals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame: usa\n",
    "usa = medals[medals.NOC == 'USA']\n",
    "\n",
    "# Group usa by ['Edition', 'Medal'] and aggregate over 'Athlete'\n",
    "usa_medals_by_year = usa.groupby(['Edition', 'Medal'])['Athlete'].count()\n",
    "\n",
    "# Reshape usa_medals_by_year by unstacking\n",
    "usa_medals_by_year = usa_medals_by_year.unstack(level='Medal')\n",
    "\n",
    "# Plot the DataFrame usa_medals_by_year\n",
    "usa_medals_by_year.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! It's difficult to gain too much insight from this visualization, however. An area plot, which you'll construct in the next exercise, may be more helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Visualizing USA Medal Counts by Edition: Area Plot</h2>\n",
    "\n",
    "As in the previous exercise, your job in this exercise is to visualize the medal counts by 'Edition' for the USA. This time, you will use an area plot to see the breakdown better. The usa DataFrame has been created and all reshaping from the previous exercise has been done. You need to write the plotting command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame: usa\n",
    "usa = medals[medals.NOC == 'USA']\n",
    "\n",
    "# Group usa by 'Edition', 'Medal', and 'Athlete'\n",
    "usa_medals_by_year = usa.groupby(['Edition', 'Medal'])['Athlete'].count()\n",
    "\n",
    "# Reshape usa_medals_by_year by unstacking\n",
    "usa_medals_by_year = usa_medals_by_year.unstack(level='Medal')\n",
    "\n",
    "# Create an area plot of usa_medals_by_year\n",
    "usa_medals_by_year.plot.area()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Visualizing USA Medal Counts by Edition: Area Plot with Ordered Medals</h2>\n",
    "\n",
    "You may have noticed that the medals are ordered according to a lexicographic (dictionary) ordering: Bronze < Gold < Silver. However, you would prefer an ordering consistent with the Olympic rules: Bronze < Silver < Gold.\n",
    "\n",
    "You can achieve this using Categorical types. In this final exercise, after redefining the 'Medal' column of the DataFrame medals, you will repeat the area plot from the previous exercise to see the new ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine 'Medal' as an ordered categorical\n",
    "medals.Medal = pd.Categorical(values=medals.Medal, categories=['Bronze', 'Silver', 'Gold'], ordered=True)\n",
    "\n",
    "# Create the DataFrame: usa\n",
    "usa = medals[medals.NOC == 'USA']\n",
    "\n",
    "# Group usa by 'Edition', 'Medal', and 'Athlete'\n",
    "usa_medals_by_year = usa.groupby(['Edition', 'Medal'])['Athlete'].count()\n",
    "\n",
    "# Reshape usa_medals_by_year by unstacking\n",
    "usa_medals_by_year = usa_medals_by_year.unstack(level='Medal')\n",
    "\n",
    "# Create an area plot of usa_medals_by_year\n",
    "usa_medals_by_year.plot.area()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have reached the end of this course - take a moment to reflect on everything you have learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
